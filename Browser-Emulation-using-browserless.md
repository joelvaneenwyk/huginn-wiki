# Browser Emulation // `browserless`

**Problem**: Certain sites have ugly source code and/or render the page using JavaScript, making it next to impossible to use the **Website Agent**. (Described in issue [#888](https://github.com/cantino/huginn/issues/888))

**Solution**: Use the **Post Agent** as the source for a **Website Agent** to scrape data from these sites. Use [browserless](http://goo-gl.me/kRTPE) to emulate the browser and return a fully rendered DOM. The **Post Agent** will use the _browserless_ API to get the rendered html of the site and send this to the **Website Agent**. This allows the **Website Agent** to then properly scrape dynamic content from JavaScript-heavy pages.

In order to use _browserless_, deploy an own instance first. See <https://github.com/browserless/chrome> for more installation instructions (Docker image available at <https://hub.docker.com/r/browserless/chrome>).

In Huginn, the **Post Agent** will request the rendered html from _browserless_ for a given url through an API call (<https://docs.browserless.io/docs/content.html>). These are the values to set in the **Post Agent**

- `post_url` - set to _browserless_url/content_ (where _browserless_url_ is wherever the instance is hosted, if using docker this will be a URL outside of both the Browserless and Huginn containers)
- `content_type` - usually set to _json_
- `method` - set to _post_
- `payload` - set to `{url: site_url}` where _site_url_ is the url of the site that should be rendered
- `emit_events` - set to _true_ (this will allow setting this agent as the source for the html of the desired site)

The remaining keys can stay at their default values.

Try a dry run to confirm that the agent returns rendered html.

**Website Agent**

1. Configure the source for this agent to be the post agent created previously.
2. Since the html for the url has already been generated by the post agent, this website agent will scrape `data_from_event` instead of `url`. The value for `data_from_event` will likely be `{{body}}`. `type` is also `html`.
3. The rest of the extraction configuration is the same as for scraping a "regular" site with the website agent ie. configure the css selectors according to the rendered html of the site being scraped.
